data:
  data_name: plbind
  data_dir: /usr/scratch/jzhu617/data
  neg_aug_p: 0.1
  use_rdkit_coords: true
  use_pocket_label: true
  pocket_cutoff: 15  # either 15 or 13 is good
  use_contact_label: false
  nearby_res_cutoff: 0
  bin_thres: 10

  feature_type: only_x # only_pos or only_x or both_x_pos for FeatEncoder, Equivariant models will use pos by default
  n_categorical_feat_to_use_lig: 1  # in {-1, 0, 1}, -1 means all categorical features
  n_scalar_feat_to_use_lig: -1  # in {-1, 0}, -1 means all scalar features
  n_categorical_feat_to_use: 1  # in {-1, 0, 1}, -1 means all categorical features
  n_scalar_feat_to_use: -1  # in {-1, 0}, -1 means all scalar features
  use_lig_info: true

logging:
  tensorboard: false
  topk: [20, 40, 60]
  # if we assume
    # 1/2 labeled amino acids are critial, then each sample on average have 60 critical amino acids
    # 1/3 labeled amino acids are critial, then each sample on average have 40 critical amino acids
    # 1/6 labeled amino acids are critial, then each sample on average have 20 critical amino acids

optimizer:
  batch_size: 128
  wp_lr: 1.0e-3
  wp_wd: 1.0e-5
  attn_lr: 1.0e-3
  attn_wd: 1.0e-5
  emb_lr: 1.0e-3
  emb_wd: 1.0e-5

egnn:
  n_layers: 4
  hidden_size: 64
  dropout_p: 0.2
  norm_type: batch
  act_type: relu
  pool: add

erm:
  warmup: 100
  epochs: 0

asap:
  warmup: 0
  epochs: 100
  casual_ratio: 0.5
  dropout_p: 0.2
  return_attn: fitness

vgib:
  epochs: 50
  warmup: 50
  mi_loss_coef: 0.00001
  reg_loss_coef: 0.1
  noise_loss_coef: 1
  dropout_p: 0.2
  norm_type: batch
  act_type: relu

ciga:
  warmup: 0
  epochs: 100
  contrast_loss_coef: 1
  hinge_loss_coef: 0.01
  causal_ratio: 0.7

  dropout_p: 0.2
  norm_type: batch
  act_type: relu

lri_bern:
  epochs: 50
  warmup: 50
  final_r: 0.5
  info_loss_coef: 0.1

  # not tuned
  one_encoder: false
  attn_constraint: smooth_min
  temperature: 1.0
  decay_interval: 10
  decay_r: 0.1
  init_r: 0.9
  pred_loss_coef: 1.0
  pred_lr: 1.0e-3
  pred_wd: 1.0e-5
  dropout_p: 0.2
  norm_type: batch
  act_type: relu

lri_gaussian:
  epochs: 50
  warmup: 50
  pos_coef: 1.2
  info_loss_coef: 0.01

  # not tuned
  kr: 7
  one_encoder: false
  attn_constraint: smooth_min
  covar_dim: 3
  pred_loss_coef: 1.0
  pred_lr: 1.0e-3
  pred_wd: 1.0e-5
  dropout_p: 0.2
  norm_type: batch
  act_type: relu

gnnlrp:
  epochs: 1

inter_grad:
  epochs: 1
  signal_class: 1

gradcam:
  epochs: 1
  signal_class: 1

gradx:
  epochs: 1
  signal_class: 1

subgraphx:
  epochs: 1
  sparsity_set: [0.3, 0.4, 0.5, 0.6, 0.7]
  score_threshold: 0.1
  high2low: false
  subgraph_building_method: "split"
  use_mcts: false
  use_pruning: true

pgexplainer:
  size_loss_coef: 0.01
  mask_ent_loss_coef: 0.1

  epochs: 50
  warmup: 100
  temp: [1.0, 1.0]
  pred_loss_coef: 1.0
  pred_lr: 0
  pred_wd: 0
  dropout_p: 0.2
  norm_type: batch
  act_type: relu

gnnexplainer:
  size_loss_coef: 0.1
  mask_ent_loss_coef: 0.1
  iter_lr: 1.0e-1

  epochs: 1
  warmup: 100
  iter_per_sample: 500
  pred_loss_coef: 1.0
  pred_lr: 0
  pred_wd: 0
  dropout_p: 0.2
  norm_type: batch
  act_type: relu

